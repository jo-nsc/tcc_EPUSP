<html lang="en">
    <head>
        <meta charset=“utf-8”/>
        <title>Graduation Project 2023</title>
        <link rel="stylesheet" href="style.css">
        <style>
            table {
                width: 100%; /* Or the width you prefer */
                border-collapse: collapse;
            }
            th, td {
                text-align: center;
                padding: 8px; /* Or the padding you prefer */
            }
        </style>
    </head>

    <body>
        <h1>Polytechnic School of the University of São Paulo</h1>
        <img src="https://www.poli.usp.br/wp-content/uploads/2020/08/Logo-Escola-Polite%CC%81cnica-Minerva_Logo-Escola-Polite%CC%81cnica-Minerva-01-scaled.jpg" id="main-image">
        <h2>Graduation Project 2023</h2>
        <h2>Resolution-wise Convolutional Neural Networks for Image Classification</h2>

        <p><strong>Student:</strong> Jônatas de Souza Nascimento</p>
        <p><strong>Advisor Professor:</strong> Prof. Dr. Artur Jordão</p>
    
        <h3>Introduction</h3>
            <p>
            Convolutional Neural Networks (CNNs) are renowned for their proficiency in computer vision tasks, particularly in image classification.
            However, advancements in CNN models coupled with the demand for higher image resolutions have led to escalated computational costs, thereby impacting their portability and the requisite computational resources. 
            This study investigates the influence of image resolution on CNN performance, seeking an equilibrium between accuracy and the associated memory and processing demands. 
            The ultimate aim is to reduce the number of network parameters, thus optimizing CNNs for use in devices with limited resources, such as mobile phones.
            </p>
          

            
            

        <h3>Problem Statement</h3>
            <p>
                <strong>Research Questions</strong>
            </p>
            <p>
                1. Is it possible to reduce the resolution of a trained Neural Network model and keep its predictive capacity? 
            </p>
            <p>
                2. Is it possible to systematically encounter a resolution for which the model keeps its predictive ability? 
            </p>
            <p>
                Answering these questions enables us to reduce the computational cost of Convolutional Neural Network based solutions by simply reducing the resolution of pre trained models 
            </p>
        
        <h3>Metodology</h3>
            <p>
                We use two well-known benchmarks, CIFAR-10 and ImageNet, and popular CNN architectures: NasNet, ResNet, and MobileNet
            </p>
            <p>
                <strong>Main Experiments</strong>
            </p>
            <p>
                1. Reduce the resolution of pre-trained CNNs and evaluate the floating points operations (FLOPs) and accuracy compared to the original model
            </p>
            <p>
                2. Select the resolution randomly from a set of pre-defined resolutions, and evaluate the FLOPs and accuracy compared to the original model
            </p>
            <p>
                <strong>Resolution Reduction Diagram</strong>
            </p>
            <figure>
                <img src="https://github.com/jo-nsc/tcc_EPUSP/blob/main/images/reducing_resolution.png?raw=true" id="reduction">
            </figure>
            
            <p>
                <strong>Random Resolution Selection Diagram</strong>
            </p>
        
            <figure>
                <img src="https://github.com/jo-nsc/tcc_EPUSP/blob/main/images/random_resolution_selector_diagram.png?raw=true" id="random">
            </figure>
            
        
        
        <h3>Results</h3>
        <p>
            <strong>NasNet on CIFAR-10</strong>
        </p>
              <table border="1">
                <tr>
                    <th>Resolution</th>
                    <th>FLOPs Drop (%)</th>
                    <th>Accuracy Drop (%)</th>
                </tr>
                <tr>
                    <td>28 x 28</td>
                    <td>23.43</td>
                    <td>1.92</td>
                </tr>
                <tr>
                    <td>24 x 24</td>
                    <td>43.74</td>
                    <td>4.34</td>
                </tr>
                <tr>
                    <td>20 x 20</td>
                    <td>60.93</td>
                    <td>13.32</td>
                </tr>
                <tr>
                    <td>16 x 16</td>
                    <td>74.99</td>
                    <td>29.58</td>
                </tr>
                <tr>
                    <td>12 x 12</td>
                    <td>85.93</td>
                    <td>52.40</td>
                </tr>
            </table>

            <p>
                <strong>ResNet on ImageNet: Comparison with existing works</strong>
            </p>

            <table border="1">
                <tr>
                    <th>Resolution</th>
                    <th>FLOPs Drop (%)</th>
                    <th>Accuracy Drop (%)</th>
                </tr>
                <tr>
                    <td>(HE, Y.; ZHANG, X.; SUN, J.)</td>
                    <td>20.00</td>
                    <td>1.70</td>
                </tr>
                <tr>
                    <td>(WANG et al., 2018)</td>
                    <td>20.00</td>
                    <td>2.00</td>
                </tr>
                <tr>
                    <td>(HE et al., 2018)</td>
                    <td>20.00</td>
                    <td>1.40</td>
                </tr>
                <tr>
                    <td>Ours (210 x 210)</td>
                    <td>3.90</td>
                    <td>1.22</td>
                </tr>
                <tr>
                    <td>Ours (196 x 196)</td>
                    <td>14.72</td>
                    <td>2.90</td>
                </tr>
                <tr>
                    <td>Ours (182 x 182)</td>
                    <td>29.26</td>
                    <td>2.70</td>
                </tr>
                <tr>
                    <td>Ours (168 x 168)</td>
                    <td>38.49</td>
                    <td>5.13</td>
                </tr>
                <tr>
                    <td>Ours (154 x 154)</td>
                    <td>49.50</td>
                    <td>6.57</td>
                </tr>
                <tr>
                    <td>Ours (140 x 140)</td>
                    <td>57.26</td>
                    <td>10.72</td>
                </tr>
                <tr>
                    <td>Ours (Random Resolution)</td>
                    <td>17.30</td>
                    <td>2.42</td>
                </tr>
            </table>

            <h3>Conclusions</h3>
            <p>
                • The experiments performed on ImageNet dataset suggest that it is
                possible to reduce the CNN resolution with minor reduction
                on its predictive capability
            </p>

            <p>
                • These experiments achieved significant reductions in FLOPs
                with a minor drop in accuracy
            </p>

            <p>
                • Random selection of model resolutions in experiments showed
                FLOPs reduction comparable to other methods in literature, with
                minimal accuracy loss
            </p>

            <p>
                • There is potential for developing a more refined method for
                selecting the scale of Neural Networks
            </p>

            
            

            <h3>References</h3>
            <p>
                [1] HE, Y ZHANG, X SUN, J Channel pruning for accelerating very deep neural networks In Proceedings of the IEEE international conference on computer vision [S.l.:s.n.], 2017
            </p>
            <p>
                [2] WANG, X et al Skipnet Learning dynamic routing in convolutional networks In Proceedings of the European Conference on Computer Vision [S.l.:s.n.], 2018
            </p>
            <p>
                [3] HE, Y et al Filter pruning via geometric median for deep convolutional neural networks acceleration In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition [S.l.:s.n.], 2019       
            </p>
            
            
    </body>

</html>
