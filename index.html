<html lang="en">
    <head>
        <meta charset=“utf-8”/>
        <title>Graduation Project 2023</title>
        <link rel="stylesheet" href="style.css">
    </head>

    <body>
        <h1>Polytechnic School of the University of São Paulo</h1>
        <img src="https://www.poli.usp.br/wp-content/uploads/2020/08/Logo-Escola-Polite%CC%81cnica-Minerva_Logo-Escola-Polite%CC%81cnica-Minerva-01-scaled.jpg">
        <h2>Graduation Project 2023</h2>
        <h2>Machine Learning Applied to Card Payment Fraud Detection</h2>

        <p><strong>Student:</strong> Jônatas de Souza Nascimento</p>
        <p><strong>Advisor Professor:</strong> Prof. Dr. Arthur Jordão</p>
    
        <h3>Introduction</h3>
            <p>
                In the realm of computer vision, Convolutional Neural Networks (CNNs) have established a stronghold, particularly excelling in tasks like image classification. 
                However, as the field advances with newer CNN models and higher image resolutions, there's an inevitable surge in computational costs. This escalation poses challenges in terms of portability and the requirements for computational resources. 
                Addressing this critical issue, our study delves into the impact of image resolution on CNNs, seeking a delicate balance between maintaining accuracy and managing the heightened demands for memory and processing. Central to our investigation is the aspiration to pare down network parameters, thereby enhancing the suitability of these networks for devices with lower computational capabilities, such as mobile phones. 
                This approach aims to make advanced CNN capabilities more accessible and efficient in less powerful devices, opening up broader possibilities for their application.
            </p>

            
            <img src="https://money.gigamundo.com/wp-content/uploads/2020/12/Credit-card-fraud-top1.jpg">

        <h3>Objective</h3>
            <p>
                Based on papers <a href="https://arxiv.org/pdf/2106.11189.pdf">1</a> , <a href="https://arxiv.org/pdf/2106.11959.pdf">2</a> and <a href="https://arxiv.org/pdf/2106.05239.pdf">3</a>, compare whether recent findings in Deep Learning (DL) for tabular data can outperform Gradient Boosted Decision Trees (GBDT), the state-of-art in this domain, considering a highly imbalanced tabular dataset.
                Moreover, it will propose an optimized model with significant efficiency in detecting fradulent card payments.
            </p>

            
            <img src="https://iili.io/HCGrOEQ.png">

        <h3>Metodology</h3>
            <p>
                In a GPU-enabled environment and using a highly imbalanced tabular dataset with millions of card payment transactions labeled as fradulent or not, a whole data pipeline was developed from scratch to train, validate, optimize, test and compare two GBDT and four DL models.
                Furthermore, several techniques, such as over sampling and adapted loss function, were discussed and used to compensate the data imbalancing.
            </p>

        <h3>Results</h3>
            <p>
                The results showed that the GBDT models outperformed so far the DL ones in all three metrics: performance by F1 score, training time and ease of code implementation.
                And after several optimization steps and techniques, the XGBoost model ended up performing greatly on the considered dataset, keeping the number of false positives low and increasing significantly the number of true positives.
            </p>

            <table>
                <caption>Results</caption>
                <tr>
                  <th>Model</th>
                  <th>TN</th>
                  <th>FP</th>
                  <th>FN</th>
                  <th>TP</th>
                  <th>F1 Score</th>
                  <th>Train Time</th>
                </tr>
                <tr>
                    <td>XGBoost</td>
                    <td>297987</td>
                    <td>643</td>
                    <td>846</td>
                    <td>524</td>
                    <td>41.31</td>
                    <td>10min 3s</td>
                </tr>
                <tr>
                    <td>LightGBM</td>
                    <td>297905</td>
                    <td>725</td>
                    <td>1074</td>
                    <td>296</td>
                    <td>24.76</td>
                    <td>6.6s</td>
                </tr>
                <tr>
                    <td>MLP</td>
                    <td>297250</td>
                    <td>1380</td>
                    <td>994</td>
                    <td>376</td>
                    <td>24.06</td>
                    <td>25min</td>
                </tr>
                <tr>
                    <td>ResNet</td>
                    <td>298535</td>
                    <td>95</td>
                    <td>1197</td>
                    <td>173</td>
                    <td>21.12</td>
                    <td>57min 13s</td>
                </tr>

                <tr>
                    <td>FTT</td>
                    <td>298625</td>
                    <td>5</td>
                    <td>1277</td>
                    <td>93</td>
                    <td>12.67</td>
                    <td>3h 13s</td>
                </tr>
                <tr>
                    <td>XBNet</td>
                    <td>298421</td>
                    <td>209</td>
                    <td>1287</td>
                    <td>83</td>
                    <td>9.90</td>
                    <td>8h</td>
                </tr>
              </table>
    </body>

</html>
