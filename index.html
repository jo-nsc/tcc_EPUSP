<html lang="en">
    <head>
        <meta charset=“utf-8”/>
        <title>Graduation Project 2023</title>
        <link rel="stylesheet" href="style.css">
    </head>

    <body>
        <h1>Polytechnic School of the University of São Paulo</h1>
        <img src="https://www.poli.usp.br/wp-content/uploads/2020/08/Logo-Escola-Polite%CC%81cnica-Minerva_Logo-Escola-Polite%CC%81cnica-Minerva-01-scaled.jpg" id="main-image">
        <h2>Graduation Project 2023</h2>
        <h2>Machine Learning Applied to Card Payment Fraud Detection</h2>

        <p><strong>Student:</strong> Jônatas de Souza Nascimento</p>
        <p><strong>Advisor Professor:</strong> Prof. Dr. Arthur Jordão</p>
    
        <h3>Introduction</h3>
            <p>
                In the realm of computer vision, Convolutional Neural Networks (CNNs) have established a stronghold, particularly excelling in tasks like image classification. 
                However, as the field advances with newer CNN models and higher image resolutions, there's an inevitable surge in computational costs. This escalation poses challenges in terms of portability and the requirements for computational resources. 
                Addressing this critical issue, our study delves into the impact of image resolution on CNNs, seeking a delicate balance between maintaining accuracy and managing the heightened demands for memory and processing. Central to our investigation is the aspiration to pare down network parameters, thereby enhancing the suitability of these networks for devices with lower computational capabilities, such as mobile phones. 
                This approach aims to make advanced CNN capabilities more accessible and efficient in less powerful devices, opening up broader possibilities for their application.
            </p>

            
            <img src="https://money.gigamundo.com/wp-content/uploads/2020/12/Credit-card-fraud-top1.jpg">

        <h3>Problem Statement</h3>

            <p>
                Research Questions
            </p>
            <p>
                1.Is it possible to reduce the resolution of a trained Neural Network model and keep its predictive capacity?
            </p>
            <p>
                2. Is it possible to systematically encounter a resolution for which the model keeps its predictive ability?
            </p>

            <p>
                Answering these questions enables us to reduce the computational cost of Convolutional Neural Network based solutions by simply reducing the resolution of pre trained models
            </p>

            <img src="https://iili.io/HCGrOEQ.png">

        <h3>Metodology</h3>
            <p>
                In a GPU-enabled environment and using a highly imbalanced tabular dataset with millions of card payment transactions labeled as fradulent or not, a whole data pipeline was developed from scratch to train, validate, optimize, test and compare two GBDT and four DL models.
                Furthermore, several techniques, such as over sampling and adapted loss function, were discussed and used to compensate the data imbalancing.
            </p>

        <h3>Results</h3>
            <p>
                The results showed that the GBDT models outperformed so far the DL ones in all three metrics: performance by F1 score, training time and ease of code implementation.
                And after several optimization steps and techniques, the XGBoost model ended up performing greatly on the considered dataset, keeping the number of false positives low and increasing significantly the number of true positives.
            </p>
              <table border="1">
                <caption>Results</caption>
                <tr>
                    <th>Resolution</th>
                    <th>FLOPs Drop (%)</th>
                    <th>Accuracy Drop (%)</th>
                </tr>
                <tr>
                    <td>28 x 28</td>
                    <td>23.43</td>
                    <td>1.92</td>
                </tr>
                <tr>
                    <td>24 x 24</td>
                    <td>43.74</td>
                    <td>4.34</td>
                </tr>
                <tr>
                    <td>20 x 20</td>
                    <td>60.93</td>
                    <td>13.32</td>
                </tr>
                <tr>
                    <td>16 x 16</td>
                    <td>74.99</td>
                    <td>29.58</td>
                </tr>
                <tr>
                    <td>12 x 12</td>
                    <td>85.93</td>
                    <td>52.40</td>
                </tr>
            </table>
    </body>

</html>
